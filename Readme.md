
### Summary

We showcase this repository as a way to conduct **internal benchmarking exercises**. 

In particular we:

1. showcase a reproducible way to compile a dataset from primary sources, 
2. provide an easy to understand summary [data card](https://github.com/AllenInstitute/bmark/blob/main/data_cards/bmark_pilot.md) for the dataset,
3. define a supervised classification task and performance metrics,
4. report results for each model (e.g. [MLP-NSF model](https://github.com/AllenInstitute/bmark/blob/main/model_cards/mlp_nsf.md), and [SCANVI](https://github.com/AllenInstitute/bmark/blob/main/model_cards/scanvi.md)) in a way that permits a fair comparisons across methods.

This effort attempts to formulate a **decentralized** approach to **continuous** benchmarking.
 - individual modelers are only responsible for tuning and submitting results for their own methods
 - new methods can be added at any time, but results can be compared in a fair manner to previously submitted methods
 - all reporting (data + model cards) is intended to be succinct yet accessible to a non-expert

We hope that this format can prevent misrepresentation of methods, have archival value through a minimal but necessary level of reproducibility, and provide the basis for informed decision making regarding choice of methods for specific tasks.

#### References

[1] [2021 Luecken et al.](https://openreview.net/forum?id=gN35BGa1Rt) <br>
[2] [2019 Mitchell et al.](https://arxiv.org/pdf/1810.03993.pdf) <br>
[3] [2021 Gebru et al.](https://arxiv.org/pdf/1803.09010.pdf) <br>

----

### Pilot benchmark dataset
 
We identified the `10Xv3 Mouse M1 data single nucleus` data generated by Allen Institute as a candidate benchmark dataset. The relevant links are below:
 - [Updated taxonomy](https://github.com/AllenInstitute/MOp_taxonomies_ontology): Links to dendrograms and hierarchy
 - [BDS google drive](https://drive.google.com/drive/folders/1SHtu-NRbJQ364VsykH2sQbfmkysrwK_TrpXHnh21S7XdTDmuBV7IH0M5OL8oCq-yJkBYerhl): Count data here is expected to match version deposited to NeMO. Metadata incorporates updates to taxonomy (compared to NeMO version) 
 - [NeMO archive](https://assets.nemoarchive.org/dat-ch1nqb7): Files for the benchmark are under `Analysis`->`BICCN_MOp_snRNA_10X_v3_Analysis_AIBS`
 - [Cell type explorer link](https://knowledge.brain-map.org/celltypes): Summary of the different Mouse M1 datasets in lower-left panel.

### Environment
```bash
conda create -n bmark
conda activate bmark
conda install python==3.8
conda install seaborn scikit-learn statsmodels numba pytables
conda install -c conda-forge python-igraph leidenalg
pip install scanpy
pip install gdown timebudget autopep8 toml sklearn
pip install jupyterlab
pip install -e .
```

 ### Pilot dataset
```bash
# Download
source scripts/download_scripts.sh
get_bmark_pilot /allen/programs/celltypes/workgroups/mousecelltypes/benchmarking/dat/pilot/

# Processing raw data with codes in ./scripts
python -m make_pilot_h5ad --data_path /allen/programs/celltypes/workgroups/mousecelltypes/benchmarking/dat/pilot --min_sample_thr 20 --write_h5ad 1
python -m make_pilot_markers --data_path /allen/programs/celltypes/workgroups/mousecelltypes/benchmarking/dat/pilot --write_csv 1
```

### Config
Create a `config.toml` file at the repository root with appropriate `data_dir` path:
```
['pilot']
data_dir = '/allen/programs/celltypes/workgroups/mousecelltypes/benchmarking/dat/pilot/'
```
 - `config.toml` is accessed through `load_config` in `bmark.utils.config`. 
 - Use `config.toml` to include any other hardcoded paths, needed for notebooks/ scripts to work correctly.

 ### Contributors
Rohan Gala, Nelson Johnson, Raymond Sanchez, Kyle Travaglini