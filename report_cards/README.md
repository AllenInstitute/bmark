# Report cards for mapping algorithms

## Overview

Detailed model evaluation on curated benchmark datasets are stored here to inform users about model biases and accuracy at mapping labels using **Allen Institute** taxonomies as reference.

## Evaluations: `human SEA-AD` benchmark

Model | Benchmark | Date | Runtime 
--- | --- | --- | --- 
[HANN](./HANN_human.md) | human MTG SEA-AD | 07/12/2023 | 3 Hours 
[HANN (FindMarkers)](./HANN_FindMarkers_human.md) | human MTG SEA-AD | 07/12/2023 | 3 Hours
[FLAT](./FLAT_human.md) | human MTG SEA-AD | 07/12/2023 | 1.8 Hours 

## Evaluations: `mouse WB` benchmark

Model | Benchmark | Date | Runtime
--- | --- | --- | --- 
HANN | mouse WB | 07/12/2023 | |
HANN (FindMarkers) | mouse WB | 07/12/2023 | | 
FLAT | mouse WB | 07/12/2023 | 0.76 | 

## Benchmarks
More details about the benchmark data can be found [here](LINK). (Link broken until data cards setup)

## Models
More details about the models benchmarked can be found [here](LINK). (Link broken until model cards setup)

## Scripts

The python scripts to produce the model report cards are hosted on the Allen Institute bmark repo.
