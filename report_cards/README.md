# Report cards for mapping algorithms

## Overview

Detailed model evaluation on curated benchmark datasets are stored here to inform users about model biases and accuracy at mapping labels from **Allen Institute** taxonomies.

## Evaluations: `human SEA-AD` benchmark

Model | Benchmark | Date | Runtime | Score [0-1]
--- | --- | --- | --- | ---
HANN | human MTG SEA-AD | 07/12/2023 | 3 Hours | 0.98
FLAT | human MTG SEA-AD | 07/12/2023 | 1.8 Hours | 0.98

## Evaluations: `mouse WB` benchmark

Model | Benchmark | Date | Runtime | Score [0-1]
--- | --- | --- | --- | ---
HANN | mouse WB | 07/12/2023 | |
FLAT | mouse WB | 07/12/2023 | |

## Benchmarks
More details about the benchmark data can be found [here](LINK)

## Models
More details about the models benchmarked can be found [here](LINK)

## Scripts

The python scripts to produce the model report cards are hosted on the Allen Institute bmark repo.
